{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZIP_DEFLATED, ZipFile\n",
    "import io\n",
    "\n",
    "\n",
    "# считываем тексты\n",
    "def read_texts():\n",
    "    data = []\n",
    "    with ZipFile('../task2/texts.zip', 'r', ZIP_DEFLATED) as zipFile:\n",
    "        for info in zipFile.infolist():\n",
    "            with io.TextIOWrapper(zipFile.open(info.filename), encoding=\"utf-8\") as f:\n",
    "                data.append(f.read())\n",
    "    print(f'posts length = {len(data)}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# читаем стоп слова\n",
    "def read_stop_words():\n",
    "    stop_words = stopwords.words(\"russian\")\n",
    "    for ch in string.punctuation:\n",
    "        stop_words.append(ch)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "# получаем токены с текста\n",
    "def get_tokens(text, stop_words):\n",
    "    tokens = word_tokenize(text.replace(\"-\", \" \"), language=\"russian\")\n",
    "    tokens = [i.lower() for i in tokens]\n",
    "    tokens = ([i for i in tokens if i not in stop_words])\n",
    "    tokens = [i for i in tokens if i != \"\" and i.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "# получаем нормальную форма слова\n",
    "def get_lemma(token):\n",
    "    return morph.parse(token)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем инвертированный список терминов из текстов\n",
    "def get_inverted_index(texts):\n",
    "    stop_words = read_stop_words()\n",
    "    terms = {}\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = get_tokens(text, stop_words)\n",
    "        lemmas = [get_lemma(token) for token in tokens]\n",
    "        for token in lemmas:\n",
    "            if terms.get(token, None) is None:\n",
    "                terms[token] = {}\n",
    "            term_doc = terms[token].get(f\"doc_{i + 1}\", None)\n",
    "            if term_doc is None:\n",
    "                term_doc = terms[token][f\"doc_{i + 1}\"] = {}\n",
    "            term_doc[\"count\"] = term_doc.get(\"count\", 0) + 1\n",
    "    return dict(sorted(terms.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# записываем индекс в файл\n",
    "def write_index(index):\n",
    "    with open('index.json', 'w', encoding='utf8') as outfile:\n",
    "        json.dump(index, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# читаем индекс\n",
    "def read_index():\n",
    "    with open('index.json', 'r', encoding='utf8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем словарь документ-число слов\n",
    "def get_doc_terms_count(texts):\n",
    "    stop_words = read_stop_words()\n",
    "    docs_count = {}\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = get_tokens(text, stop_words)\n",
    "        lemmas = [get_lemma(token) for token in tokens]\n",
    "        docs_count[f\"doc_{i + 1}\"] = len(lemmas)\n",
    "    return docs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_doc_terms_count(docs_count):\n",
    "    with open('doc_count.json', 'w', encoding='utf8') as outfile:\n",
    "        json.dump(docs_count, outfile, indent=4, ensure_ascii=False)\n",
    "        \n",
    "def read_doc_terms_count():\n",
    "    with open('doc_count.json', 'r', encoding='utf8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисляем tf для каждого термина и документа\n",
    "def compute_tfs(index, terms_count):\n",
    "    for key, term in index.items():\n",
    "        for doc_key, doc in term.items():\n",
    "            doc[\"tf\"] = compute_tf(doc[\"count\"], terms_count[doc_key])\n",
    "    return index\n",
    "\n",
    "def compute_tf(term_count, all_terms_count):\n",
    "    return round(term_count / float(all_terms_count), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# вычисляем idf для каждого термина\n",
    "def compute_idfs(index, D):\n",
    "    for key, term in index.items():\n",
    "        term_docs_count = len(term.keys())\n",
    "        idf = compute_idf(D, term_docs_count)\n",
    "        for doc_key, doc in term.items():\n",
    "            doc[\"idf\"] = idf\n",
    "    return index\n",
    "\n",
    "\n",
    "def compute_idf(all_docs, docs_count):\n",
    "    return round(math.log10(all_docs / float(docs_count)), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисляем tf-idf для каждого термина и документа\n",
    "def compute_tfs_idfs(index):\n",
    "    for key, term in index.items():\n",
    "        for doc_key, doc in term.items():\n",
    "            doc[\"tf-idf\"] = compute_tf_idf(doc[\"tf\"], doc[\"idf\"])\n",
    "    return index\n",
    "\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return round(tf * idf, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# записываем значения tf, idf, tf-idf в json и txt(по заданию)\n",
    "def write_terms_with_tf_idf(index):\n",
    "    with open('tf_idf.json', 'w', encoding='utf8') as outfile:\n",
    "        json.dump(index, outfile, indent=4, ensure_ascii=False)\n",
    "    with open('tf_idf.txt', \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, term in index.items():\n",
    "            file.write(f'{key}\\n')\n",
    "            for doc_key, doc in term.items():\n",
    "                file.write(\"\" * len(key) + f'{doc_key} tf={doc[\"tf\"]} idf={doc[\"idf\"]} tf-idf={doc[\"tf-idf\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts length = 100\n"
     ]
    }
   ],
   "source": [
    "# вычисляем индекс по текстам и кол-во слов в каждом тексте\n",
    "texts = read_texts()\n",
    "inverted_index = get_inverted_index(texts)\n",
    "docs_terms_count = get_doc_terms_count(texts)\n",
    "# записываем значения на будущее\n",
    "write_index(inverted_index)\n",
    "write_doc_terms_count(docs_terms_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D - кол-во текстов\n",
    "D = 100\n",
    "# считываем данные, вычисляем значения и записываем их\n",
    "index = read_index()\n",
    "doc_terms_count = read_doc_terms_count()\n",
    "index = compute_tfs(index, doc_terms_count)\n",
    "index = compute_idfs(index, D)\n",
    "index = compute_tfs_idfs(index)\n",
    "write_terms_with_tf_idf(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
